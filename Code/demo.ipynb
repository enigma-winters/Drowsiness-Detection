{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils.video import VideoStream\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import pickle\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import dlib\n",
    "from scipy.spatial import distance as dist\n",
    "x = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading face detector\n",
      "loading the liveness detector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:311: UserWarning: Trying to unpickle estimator LabelEncoder from version 0.20.2 when using version 0.19.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "print(\"loading face detector\")\n",
    "protoPath =  'deploy.prototxt'\n",
    "# os.path.sep.join([args[\"detector\"], \"deploy.prototxt\"])\n",
    "#Loading the caffe model \n",
    "modelPath = 'res10_300x300_ssd_iter_140000.caffemodel'\n",
    "# os.path.sep.join([args[\"detector\"],\n",
    "# \t\"res10_300x300_ssd_iter_140000.caffemodel\"])\n",
    "#reading data from the model.\n",
    "net = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
    "\n",
    "# loading the liveness detecting module that was trained in the training python script\n",
    "print(\"loading the liveness detector\")\n",
    "model = load_model('liveness.model')\n",
    "le = pickle.loads(open('le.pickle', \"rb\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determining the facial points that are plotted by dlib\n",
    "FULL_POINTS = list(range(0, 68))  \n",
    "FACE_POINTS = list(range(17, 68))  \n",
    "   \n",
    "JAWLINE_POINTS = list(range(0, 17))  \n",
    "RIGHT_EYEBROW_POINTS = list(range(17, 22))  \n",
    "LEFT_EYEBROW_POINTS = list(range(22, 27))  \n",
    "NOSE_POINTS = list(range(27, 36))  \n",
    "RIGHT_EYE_POINTS = list(range(36, 42))  \n",
    "LEFT_EYE_POINTS = list(range(42, 48))  \n",
    "MOUTH_OUTLINE_POINTS = list(range(48, 61))  \n",
    "MOUTH_INNER_POINTS = list(range(61, 68))  \n",
    "   \n",
    "EYE_AR_THRESH = 0.30 \n",
    "EYE_AR_CONSEC_FRAMES = 2  \n",
    "\n",
    "#initializing the parameters\n",
    "COUNTER_LEFT = 0  \n",
    "TOTAL_LEFT = 0  \n",
    "   \n",
    "COUNTER_RIGHT = 0  \n",
    "TOTAL_RIGHT = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function for calculating ear and then comparing with the confidence parametrs\n",
    "\n",
    "def eye_aspect_ratio(eye):\n",
    "    \n",
    "    A = dist.euclidean(eye[1], eye[5])  \n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    C = dist.euclidean(eye[0], eye[3])  \n",
    "    ear = (A + B) / (2.0 * C)  \n",
    "    return ear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the predictor for predicting\n",
    "detector = dlib.get_frontal_face_detector()  \n",
    "\n",
    "#accessing the shape predictor\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left eye winked\n",
      "Right eye winked\n",
      "Left eye winked\n",
      "Left eye winked\n",
      "Right eye winked\n",
      "Left eye winked\n",
      "Right eye winked\n",
      "Left eye winked\n",
      "Right eye winked\n",
      "Left eye winked\n",
      "Right eye winked\n",
      "Right eye winked\n",
      "Left eye winked\n",
      "Right eye winked\n",
      "Right eye winked\n",
      "Left eye winked\n",
      "Right eye winked\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f383c4299346>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTOTAL_LEFT\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mTOTAL_RIGHT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     \u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m     temp = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0,\n\u001b[0;32m     61\u001b[0m \t\t(300, 300), (104.0, 177.0, 123.0))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "#looping over frames\n",
    "while True:\n",
    "    #checkpoint 1\n",
    "    ret, frame = video_capture.read()\n",
    "    if ret:\n",
    "        \n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  \n",
    "        rects = detector(gray, 0)\n",
    "        frame = imutils.resize(frame, width=600)\n",
    "        for rect in rects:\n",
    "            \n",
    "            x = rect.left()  \n",
    "            y = rect.top()  \n",
    "            x1 = rect.right()  \n",
    "            y1 = rect.bottom()\n",
    "            landmarks = np.matrix([[p.x, p.y] for p in predictor(frame, rect).parts()])  \n",
    "            left_eye = landmarks[LEFT_EYE_POINTS]  \n",
    "            right_eye = landmarks[RIGHT_EYE_POINTS]  \n",
    "            left_eye_hull = cv2.convexHull(left_eye)  \n",
    "            right_eye_hull = cv2.convexHull(right_eye)  \n",
    "            ear_left = eye_aspect_ratio(left_eye)  \n",
    "            ear_right = eye_aspect_ratio(right_eye)\n",
    "\t\t\n",
    "            #calculating blink wheneer the ear value drops down below the threshold\n",
    "\t\n",
    "            if ear_left < EYE_AR_THRESH:\n",
    "                \n",
    "                COUNTER_LEFT += 1\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                \n",
    "                if COUNTER_LEFT >= EYE_AR_CONSEC_FRAMES:\n",
    "                    \n",
    "                    \n",
    "                    TOTAL_LEFT += 1  \n",
    "                    print(\"Left eye winked\") \n",
    "                \n",
    "                    COUNTER_LEFT = 0\n",
    "            if ear_right < EYE_AR_THRESH:  \n",
    "                \n",
    "                \n",
    "                COUNTER_RIGHT += 1  \n",
    "\n",
    "            else:\n",
    "                \n",
    "                if COUNTER_RIGHT >= EYE_AR_CONSEC_FRAMES: \n",
    "                    \n",
    "                    \n",
    "                    TOTAL_RIGHT += 1  \n",
    "                    print(\"Right eye winked\")  \n",
    "                    COUNTER_RIGHT = 0\n",
    "\n",
    "\n",
    "            x = TOTAL_LEFT + TOTAL_RIGHT\n",
    "\n",
    "    (h, w) = frame.shape[:2]\n",
    "    temp = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0,\n",
    "\t\t(300, 300), (104.0, 177.0, 123.0))\n",
    "    net.setInput(temp)\n",
    "    detections = net.forward()\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        \n",
    "        \n",
    "        confidence = detections[0, 0, i, 2]\n",
    "            \n",
    "          #staisfying the union need of veryfying through ROI and blink detection.  \n",
    "        if confidence > 0.5 and x>10:\n",
    "            \n",
    "            \n",
    "             \n",
    "            #detect a bounding box\n",
    "\t    #take dimensions\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "\t    #get the dimensions\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "\t\t\t\n",
    "            startX = max(0, startX)\n",
    "            startY = max(0, startY)\n",
    "            endX = min(w, endX)\n",
    "            endY = min(h, endY)\n",
    "\n",
    "\t# extract the face ROI and then preproces it in the exact\n",
    "\t# same manner as our training data\n",
    "            face = frame[startY:endY, startX:endX]\n",
    "            face = cv2.resize(face, (32, 32))\n",
    "            face = face.astype(\"float\") / 255.0\n",
    "            face = img_to_array(face)\n",
    "            face = np.expand_dims(face, axis=0)\n",
    "\n",
    "\t#pass the model to determine the liveness\n",
    "            preds = model.predict(face)[0]\n",
    "            j = np.argmax(preds)\n",
    "            label = le.classes_[1-j]\n",
    "\n",
    "\t\t# tag with the label\n",
    "\t\t#tag with the bounding box\n",
    "            label = \"{}: {:.4f}\".format(label, preds[j])\n",
    "            cv2.putText(frame, label, (startX, startY - 10),\n",
    "\t\t\t\t cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY),\n",
    "\t\t\t\t  (0, 0, 255), 2)\n",
    " #showing the frames and waiting for the key to be pressed\n",
    "            cv2.imshow(\"Frame\", frame)\n",
    "    \n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord(\"q\"):\n",
    "                video_capture.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                break\n",
    "#vs.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
